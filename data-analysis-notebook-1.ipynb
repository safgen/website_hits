{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis and Feature encoding\n",
    "\n",
    "Before, moving on to the prediction step, the data needs to be analyzed first. This document presents the following stages of this case study.\n",
    "1. Examining the data\n",
    "2. Managiing the missing Values\n",
    "3. Visualizing individual Data Columns\n",
    "4. Cleaning the data\n",
    "4. Checking Correlations\n",
    "5. Feature Encoding\n",
    "6. Separating the known and unknown 'hits' data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, let's import all the packages we'll be needing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, Imputer, LabelEncoder, MinMaxScaler\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction import FeatureHasher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's have a look at the data set as a whole to understand the schema of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(988681, 10)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 988681 entries, 0 to 988680\n",
      "Data columns (total 10 columns):\n",
      "row_num              988681 non-null int64\n",
      "locale               988681 non-null object\n",
      "day_of_week          988681 non-null object\n",
      "hour_of_day          988681 non-null int64\n",
      "agent_id             988681 non-null int64\n",
      "entry_page           988681 non-null int64\n",
      "path_id_set          983792 non-null object\n",
      "traffic_type         988681 non-null int64\n",
      "session_durantion    988681 non-null object\n",
      "hits                 988681 non-null object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 75.4+ MB\n"
     ]
    }
   ],
   "source": [
    "nRowsRead = None  # This can be changed into an integer in order to load a smaller chunk of the data.\n",
    "df = pd.read_csv('ML Data Scientist Case Study Data.csv', delimiter= ';', nrows = nRowsRead)\n",
    "print(df.shape)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a typo in one of the the column names, so let's correct this in order to avoid confusion later. Also, let's replace the unknown values with numpy nans for ease of  handlimg the unknown values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 988681 entries, 0 to 988680\n",
      "Data columns (total 10 columns):\n",
      "row_num             988681 non-null int64\n",
      "locale              988681 non-null object\n",
      "day_of_week         988681 non-null object\n",
      "hour_of_day         988681 non-null int64\n",
      "agent_id            988681 non-null int64\n",
      "entry_page          988681 non-null int64\n",
      "path_id_set         983792 non-null object\n",
      "traffic_type        988681 non-null int64\n",
      "session_duration    988013 non-null object\n",
      "hits                619235 non-null object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 75.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# fixing the typo in the column name\n",
    "df = df.rename(columns={'session_durantion': 'session_duration'})\n",
    "# replacing missing values with Nan\n",
    "df = df.replace('\\\\N', np.nan)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "row_num                  0\n",
       "locale                   0\n",
       "day_of_week              0\n",
       "hour_of_day              0\n",
       "agent_id                 0\n",
       "entry_page               0\n",
       "path_id_set           4889\n",
       "traffic_type             0\n",
       "session_duration       668\n",
       "hits                369446\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that the couple of feature columns also hae missing values. In the following section, let's tackle those those values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill in the missing values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by filling in the missing values in session_duration by using a simple imputer with a mean strategy. We could have used an iterative imputer but that seemed to be an overkill for such a small number of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "row_num                  0\n",
       "locale                   0\n",
       "day_of_week              0\n",
       "hour_of_day              0\n",
       "agent_id                 0\n",
       "entry_page               0\n",
       "path_id_set           4889\n",
       "traffic_type             0\n",
       "session_duration         0\n",
       "hits                369446\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\n",
    "imputer = imputer.fit(df[['session_duration']])\n",
    "df['session_duration'] = imputer.transform(df[['session_duration']])\n",
    "df['session_duration'] = df['session_duration'].astype(float) # changing to a numeric type after filling in the missing values \n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values filled in. Now, let's deal with the missing path_id_set values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     4889.000000\n",
      "mean       905.935570\n",
      "std       6441.779028\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max      82462.000000\n",
      "Name: session_duration, dtype: float64\n",
      "count     3153\n",
      "unique      44\n",
      "top          1\n",
      "freq      2743\n",
      "Name: hits, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# analyzing the entries with missing path ids\n",
    "df_no_path = df[df['path_id_set'].isna()]\n",
    "print(df_no_path['session_duration'].describe())\n",
    "print(df_no_path['hits'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The session_duration and hits values where path_ids are zeros indicate that missing path_ids can be assumed as an 'empty set' of values rather than missing values. Also, changing the 'hits' column to numeric type after changing '\\N' to np.nan earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hits'] = df['hits'].astype(float) # changing to numeric type; float because of numpy nans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing individual columns\n",
    "\n",
    "Here let's just visualize individual features of the data to get a better unserstanding of what we are dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['locale'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day_of_week'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour_of_day'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['agent_id'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entry_page'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['traffic_type'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['session_duration'].describe())\n",
    "df['session_duration'].plot(kind='hist', bins=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The description of this column and its distribution tells us that there may be a very small fraction values that are skewing the picture and are not representative of the data in general. Let's deal with it in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['path_id_set'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['hits'].describe())\n",
    "df['hits'].plot(kind='hist', bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here again, we see a small number of outliers skewing the overall representation disproportionately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "Let's remove the extreme outliers. I'll be using quantiles to remove the extreme outliers. The model training improved drastically with this simple, yet effective, filtering on the two columns from the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = df['session_duration'].quantile(0.98)\n",
    "q2 = df['hits'].quantile(0.98)\n",
    "\n",
    "df_clean = df.copy()\n",
    "df_empty = df_clean[df_clean['hits'].isna()]\n",
    "df_not_empty = df_clean[df_clean['hits'].notna()]\n",
    "\n",
    "df_clean = df_not_empty[df_not_empty['session_duration'] < q1]\n",
    "df_clean = df_clean[df_clean['hits'] < q2]\n",
    "df_clean = pd.concat([df_clean, df_empty], axis=0)\n",
    "print(df_clean.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, the correlation analysis - described later in this notebook - revealed that the number of hits have a higher correlation with the the number of locations in the path_id_set rather than the actual locations themselves. So we'll add an additional column to help us build a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping function to get the path_length from path_id_set\n",
    "def get_path_length(x):\n",
    "    if x is not np.nan:\n",
    "        x = str(x)\n",
    "        y = len(x.split(';'))\n",
    "    else:\n",
    "        y=0\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['path_length'] = df_clean['path_id_set'].apply(lambda x: get_path_length(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, now we have a new column in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlations\n",
    "\n",
    "Before checking correlations, we'll separate data with hits so that np.nans don't interfere with the correlation calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_hits = df_clean[df_clean['hits'].isna()]\n",
    "df_hits = df_clean[df_clean['hits'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's observe the Correlation among continuous variables after path_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_col = ['session_duration', 'path_length', 'hits']\n",
    "df_cont = df_hits[cont_col] \n",
    "corr = df_cont.corr(method='spearman')\n",
    "corr.style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's observe the  Correlation Ratio between hits and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_ratio(categories, measurements):\n",
    "    fcat, _ = pd.factorize(categories)\n",
    "    cat_num = np.max(fcat)+1\n",
    "    y_avg_array = np.zeros(cat_num)\n",
    "    n_array = np.zeros(cat_num)\n",
    "    for i in range(0,cat_num):\n",
    "        cat_measures = measurements[np.argwhere(fcat == i).flatten()]\n",
    "        n_array[i] = len(cat_measures)\n",
    "        y_avg_array[i] = np.average(cat_measures)\n",
    "    y_total_avg = np.sum(np.multiply(y_avg_array,n_array))/np.sum(n_array)\n",
    "    numerator = np.sum(np.multiply(n_array,np.power(np.subtract(y_avg_array,y_total_avg),2)))\n",
    "    denominator = np.sum(np.power(np.subtract(measurements,y_total_avg),2))\n",
    "    if numerator == 0:\n",
    "        eta = 0.0\n",
    "    else:\n",
    "        eta = np.sqrt(numerator/denominator)\n",
    "    return eta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cols in df_hits.columns:\n",
    "    if cols not in ('session_duration', 'path_length', 'hits', 'row_num', 'path_id_set'):\n",
    "        cor_ratio = correlation_ratio(df_hits[cols].tolist(), np.array(df_hits['hits'].tolist()))\n",
    "        print (\"Correlation ratio between \" + cols + \" & hits is = \" + str(cor_ratio) + \n",
    "               ', whereas total # of categories is: ' + str(len(df_hits[cols].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_col = ['day_of_week', 'locale', 'agent_id', 'entry_page', 'traffic_type', 'hour_of_day', 'hits']\n",
    "\n",
    "df_plot = df_hits[plot_col]\n",
    "sns.pairplot(df_plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure above shows us the pairwise plot of different features of our data. The next step after the understading of the data is to encode this data so that it can be fed into a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before encoding, let's view how our cleaned DataFrame looks like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature encoding\n",
    "\n",
    "The feature encoding is a very immportant part for preparing the data for the model training and model prediction. So, getting the right kind of encoding for each feature is very important. This dataset is particularly challengng in that regard because it has features that are\n",
    "1) nominal categorical\n",
    "2) categorical with large number of categories\n",
    "3) continous numerical\n",
    "4) cyclical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's start by converting all the nominal categorical variables to one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df_clean.copy() # getting a new DataFrame instance for encodings\n",
    "\n",
    "for i in ('locale', 'agent_id', 'traffic_type'):\n",
    "    dummies = pd.get_dummies(df_encoded[i], drop_first=True,  prefix=i.split('_')[0])\n",
    "    df_encoded = pd.concat([df_encoded, dummies], axis=1)\n",
    "    df_encoded = df_encoded.drop(columns=[i]) \n",
    "df_encoded.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have two categorical features with a very large number of discrete categories. Using one-hot-encoding here will lead us to a bad place, named curse of dimensionality.\n",
    "So, instead well limit the encodings for these features to 10 by using FeatureHasher for entry_page while we have already replaced path_id_set by path_length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasher = FeatureHasher(n_features=10, input_type='string')\n",
    "hashed_features = hasher.fit_transform(df_encoded['entry_page'].astype(str))\n",
    "hashed_features = hashed_features.toarray()\n",
    "df_encoded = pd.concat([df_encoded.drop(columns=['entry_page']), pd.DataFrame(hashed_features, \n",
    "                                                                 columns=['ep_0', 'ep_1', 'ep_2', 'ep_3', 'ep_4', \n",
    "                                                                          'ep_5', 'ep_6', 'ep_7',\n",
    "                                                                          'ep_8', 'ep_9'])], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.drop(columns=['path_id_set'], inplace=True)\n",
    "df_encoded.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding hashed features, we're done with the categorical feature. \n",
    "Now we'll scale the contionuous features. various options were explored but the simplest option of MinMaxScaler worked as well as any other "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler  = StandardScaler()\n",
    "#minmax = MinMaxScaler()\n",
    "#df_encoded[['path_length', 'session_duration']] = scaler.fit_transform(df_encoded[['path_length', 'session_duration']].apply(lambda x: np.log(1+x)))\n",
    "#df_encoded[['path_length', 'session_duration']] = minmax.fit_transform(df_clean[['path_length', 'session_duration']])\n",
    "\n",
    "#df_encoded[['path_length', 'session_duration']].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For days and hours, we have to use cysclic encoding because they repeat periodically. Following cyclic method was used for encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_mapper  = dict(zip(['Sunday','Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday'], \n",
    "                       [0, 1, 2, 3 ,4, 5, 6]))\n",
    "\n",
    "df_encoded['day_of_week_enc'] = df_encoded['day_of_week'].map(day_mapper)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_encoded['day_sin'] =  np.sin(2*np.pi*df_encoded.day_of_week_enc/7)\n",
    "df_encoded['day_cos'] =  np.cos(2*np.pi*df_encoded.day_of_week_enc/7)\n",
    "\n",
    "df_encoded['hour_sin'] =  np.sin(2*np.pi*df_encoded.hour_of_day/24)\n",
    "df_encoded['hour_cos'] =  np.cos(2*np.pi*df_encoded.hour_of_day/24)\n",
    "\n",
    "df_encoded[['day_cos', 'day_sin']].describe()\n",
    "df_encoded[['hour_cos', 'hour_sin']].describe()\n",
    "\n",
    "df_encoded = df_encoded.drop(columns= ['day_of_week', 'day_of_week_enc', 'hour_of_day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's save the encoded files into csv files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.to_csv('feature_engineered_data_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded[df_encoded['hits'].notna()].to_csv('feature_engineered_data_with_hits_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded[df_encoded['hits'].isna()].to_csv('feature_engineered_data_without_hits_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded[df_encoded['hits'].isna()].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
